{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandonmoss124/mgmt467-analytics-portfolio/blob/main/lab8_streaming_pipeline_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bd5d907c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd5d907c",
        "outputId": "5bad49cd-5d78-475f-b100-b1f45c2165ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/321.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Setup: install Pub/Sub client library in Colab\n",
        "!pip install -q google-cloud-pubsub functions-framework\n",
        "\n",
        "from google.colab import auth  # comment out if not using Colab\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33f5c779",
      "metadata": {
        "id": "33f5c779"
      },
      "source": [
        "# Lab 8: Building a Streaming Pipeline (Pub/Sub + Dataflow)\n",
        "\n",
        "This notebook documents your steps to convert the batch pipeline from Lab 7\n",
        "into a streaming pipeline using Pub/Sub and a Dataflow template.\n",
        "\n",
        "Use this as a structured template; you still need to perform the steps\n",
        "in the GCP console and customize resource names."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d5beded",
      "metadata": {
        "id": "0d5beded"
      },
      "source": [
        "## 1. Pub/Sub Topic Setup\n",
        "\n",
        "In the GCP Console, create a topic named `live-data-stream`.\n",
        "\n",
        "**TODO:** Document the exact topic path here (e.g., `projects/your-project/topics/live-data-stream`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec8b3dd",
      "metadata": {
        "id": "7ec8b3dd"
      },
      "source": [
        "## 2. Modify Cloud Function to Publish to Pub/Sub\n",
        "\n",
        "Update your Lab 7 Cloud Function so that instead of writing directly to\n",
        "BigQuery, it publishes the weather JSON to the Pub/Sub topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "47a150ee",
      "metadata": {
        "id": "47a150ee"
      },
      "outputs": [],
      "source": [
        "from google.cloud import pubsub_v1\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "import functions_framework\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# TODO: update this to your actual project ID and topic name\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"proven-agility-477721-q9\")\n",
        "TOPIC_ID = \"live-data-stream\"\n",
        "\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "TOPIC_PATH = publisher.topic_path(PROJECT_ID, TOPIC_ID)\n",
        "\n",
        "def publish_weather(weather_dict):\n",
        "    \"\"\"Publish a weather dict as JSON to Pub/Sub.\"\"\"\n",
        "    data_str = json.dumps(weather_dict)\n",
        "    future = publisher.publish(TOPIC_PATH, data=data_str.encode(\"utf-8\"))\n",
        "    message_id = future.result()\n",
        "    logger.info(f\"Published message ID: {message_id}\")\n",
        "\n",
        "@functions_framework.http\n",
        "def ingest_weather_producer(request):\n",
        "    \"\"\"HTTP Cloud Function that fetches weather and publishes to Pub/Sub.\n",
        "\n",
        "    NOTE: This expects you to have defined `fetch_weather_lafayette()`\n",
        "    (from Lab 7) in the same source file.\n",
        "    \"\"\"\n",
        "    weather_json = fetch_weather_lafayette()\n",
        "    publish_weather(weather_json)\n",
        "    return (\"OK: message published\", 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abfc332b",
      "metadata": {
        "id": "abfc332b"
      },
      "source": [
        "## 3. Dataflow Job from Template\n",
        "\n",
        "In the GCP Console:\n",
        "1. Go to **Dataflow → Create job from template**.\n",
        "2. Select the template **Pub/Sub Topic to BigQuery**.\n",
        "3. Configure parameters:\n",
        "   * **Input topic**: your `live-data-stream` topic.\n",
        "   * **Output table**: a new table in your dataset, e.g.,\n",
        "     `your-project.superstore_data.realtime_weather_streaming`.\n",
        "   * **Temp location**: a folder in a GCS bucket, e.g.,\n",
        "     `gs://your-bucket/dataflow-temp/`.\n",
        "\n",
        "**TODO:** After you launch the job, include a screenshot of the running graph in your lab submission."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab62bd0",
      "metadata": {
        "id": "aab62bd0"
      },
      "source": [
        "## 4. Validate Streaming Data in BigQuery\n",
        "\n",
        "Trigger your Cloud Function manually (via HTTP), wait for Dataflow to process,\n",
        "and then query your output table. Example query (adapt to your table name):\n",
        "\n",
        "```sql\n",
        "SELECT *\n",
        "FROM `your-project.superstore_data.realtime_weather_streaming`\n",
        "ORDER BY dt_utc DESC\n",
        "LIMIT 10;\n",
        "```\n",
        "\n",
        "**TODO:** Capture a screenshot of the query results showing streamed rows."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc79a560",
      "metadata": {
        "id": "cc79a560"
      },
      "source": [
        "## 5. Challenge: Apache Beam Concepts Prompt\n",
        "\n",
        "Author a prompt to ask Gemini to explain the three core concepts of Apache Beam\n",
        "(`Pipeline`, `PCollection`, and `ParDo`) using an analogy (e.g., an assembly line).\n",
        "\n",
        "Record your prompt and Gemini's response here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0de860e",
      "metadata": {
        "id": "d0de860e"
      },
      "source": [
        "Using a factory assembly line as an analogy, explain the three core concepts of Apache Beam: Pipeline, PCollection, and ParDo — what each one represents and how they work together.\n",
        "Imagine Apache Beam as a factory assembly line that processes products:\n",
        "\n",
        "Pipeline The conveyor belt / full assembly line\n",
        "It defines the entire workflow and the order of operations — where the data starts and how it moves.\n",
        "\n",
        "PCollection The items on the belt\n",
        "These represent the data elements being processed, like individual products traveling through the factory.\n",
        "\n",
        "ParDo The workers at each station\n",
        "Each ParDo performs a specific task on every item it receives — such as cutting, painting, labeling, etc.\n",
        "\n",
        "Together, the Pipeline path, PCollections items, and ParDo workers enable data to move through a well-structured process, transforming raw data into useful output — just like a real production line."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}