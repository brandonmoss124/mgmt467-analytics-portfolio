{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandonmoss124/mgmt467-analytics-portfolio/blob/main/Lab9_Final_Project_Blueprint_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f415dd6",
      "metadata": {
        "id": "9f415dd6"
      },
      "source": [
        "# Final Project Blueprint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cab749",
      "metadata": {
        "id": "c7cab749"
      },
      "source": [
        "## Section 1 — Business Problem\n",
        "\n",
        "I want to build a near real-time weather-risk monitoring tool for a small delivery company operating in Indiana.\n",
        "Dispatchers need to know when severe weather (heavy rain, snow, or high winds) is likely to affect on-time deliveries\n",
        "in the next few hours so they can reroute drivers or adjust schedules. Historical weather and delay data will be used\n",
        "to understand patterns, while live weather data will give an up-to-date view of current risk. Success will be measured\n",
        "by reduced late deliveries during bad weather and by how quickly dispatchers can see and react to changing conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd19306",
      "metadata": {
        "id": "5cd19306"
      },
      "source": [
        "## Section 2 — Data Sources\n",
        "\n",
        "### 2.1 Batch Source\n",
        "- **Source name / URL:** Kaggle “US Weather Events (2016–2021)” CSV export\n",
        "- **File format & storage location:** Multiple CSV files uploaded to `gs://proven-agility-477721-q9-bucket/weather_batch/`\n",
        "- **Rough size:** ~6 million rows, 6 years of data (2016–2021) across the U.S.\n",
        "- **Important fields and grain:**\n",
        "  - Grain: one row per weather event (e.g., `Rain`, `Snow`, `Fog`) in a given location and time interval\n",
        "  - Fields: `event_type`, `severity`, `start_time`, `end_time`, `city`, `state`, `latitude`, `longitude`\n",
        "\n",
        "### 2.2 Streaming Source\n",
        "- **Public API name and endpoint:** OpenWeatherMap Current Weather API (`https://api.openweathermap.org/data/2.5/weather`)\n",
        "- **How often new data arrives:** Cloud Scheduler will trigger the pipeline every 5 minutes.\n",
        "- **Important fields I will use:** `dt` (timestamp), `main.temp`, `main.humidity`, `wind.speed`,\n",
        "  `weather[0].main`, `weather[0].description`\n",
        "- **Landing path:** `OpenWeather API → HTTP Cloud Function (ingest_weather_producer) → Pub/Sub topic (live-data-stream)\n",
        "  → Dataflow template (Pub/Sub to BigQuery) → BigQuery table: `proven-agility-477721-q9.superstore_data.realtime_weather_streaming`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c89ac10f",
      "metadata": {
        "id": "c89ac10f"
      },
      "source": [
        "## Section 3 — Cloud Architecture\n",
        "\n",
        "### 3.1 Text Architecture\n",
        "\n",
        "- **Batch path:**\n",
        "\n",
        "  `Kaggle CSV files → GCS bucket (weather_batch) → BigQuery dataset superstore_data → table historical_weather`\n",
        "\n",
        "- **Streaming path:**\n",
        "\n",
        "  `OpenWeather API → Cloud Function (ingest_weather_producer) → Pub/Sub topic live-data-stream →`\n",
        "  `Dataflow streaming job (Pub/Sub to BigQuery template) → BigQuery table realtime_weather_streaming`\n",
        "\n",
        "- **ML + Analytics:**\n",
        "\n",
        "  `historical_weather + realtime_weather_streaming (joined on location & time bucket) →`\n",
        "  `BigQuery ML model (weather_delay_risk_model) → prediction table weather_risk_scores`\n",
        "\n",
        "- **Dashboard:**\n",
        "\n",
        "  `BigQuery tables (realtime_weather_streaming, weather_risk_scores) → Looker Studio dashboard (live risk map + trends)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfeabfd1",
      "metadata": {
        "id": "bfeabfd1"
      },
      "source": [
        "### 3.2 Optional Diagram Link\n",
        "\n",
        "_Example placeholder_: Link to architecture diagram stored in Google Drive or as an image in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05811621",
      "metadata": {
        "id": "05811621"
      },
      "source": [
        "### Gemini Architecture Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f1b327d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "f1b327d5",
        "outputId": "6f2a2353-4dd9-475e-e93c-44ef7edf2ce3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# TASK: Act as a Google Cloud Solution Architect.\\n# CONTEXT: I am building a project to monitor near real-time weather risk for a small delivery company in Indiana.\\n# DATA SOURCES: A batch CSV of historical weather events (uploaded to BigQuery) and a streaming API (OpenWeather)\\n# that sends current weather data every few minutes.\\n# GOAL: A Looker Studio dashboard showing current high-risk areas plus historical patterns of bad weather and delays.\\n# REQUEST: Design a simple GCP architecture using services like Cloud Functions, Pub/Sub, Dataflow, BigQuery,\\n#          and BigQuery ML. Explain the specific role each service plays in this pipeline and how they connect together.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "# TASK: Act as a Google Cloud Solution Architect.\n",
        "# CONTEXT: I am building a project to monitor near real-time weather risk for a small delivery company in Indiana.\n",
        "# DATA SOURCES: A batch CSV of historical weather events (uploaded to BigQuery) and a streaming API (OpenWeather)\n",
        "# that sends current weather data every few minutes.\n",
        "# GOAL: A Looker Studio dashboard showing current high-risk areas plus historical patterns of bad weather and delays.\n",
        "# REQUEST: Design a simple GCP architecture using services like Cloud Functions, Pub/Sub, Dataflow, BigQuery,\n",
        "#          and BigQuery ML. Explain the specific role each service plays in this pipeline and how they connect together.\n",
        "\"\"\"\n",
        "prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9273f7d1",
      "metadata": {
        "id": "9273f7d1"
      },
      "source": [
        "Gemini said my design is a good fit for what I want to do. It explained that the Cloud Function is a simple way to collect data from the OpenWeather API. Pub/Sub helps make sure messages don’t get lost and lets Dataflow handle the streaming data separately. Dataflow keeps sending the live weather data into BigQuery, where I can also store my batch weather data. Gemini also said BigQuery ML is the right place to train and run my prediction model so everything stays in one system. It recommended using monitoring and alerts in case the API has problems or the stream stops working.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d256ba",
      "metadata": {
        "id": "e7d256ba"
      },
      "source": [
        "## Section 4 — Machine Learning Plan (BigQuery ML)\n",
        "\n",
        "- **Problem type:** Classification — predict whether weather conditions are **High-Risk** vs **Normal**\n",
        "  for delivery delays in a given area and time slot.\n",
        "- **Label (target variable):** `is_high_risk` (boolean), derived from historical data where severe weather\n",
        "  corresponded to high numbers of late deliveries.\n",
        "- **Features from batch table (`historical_weather`):**\n",
        "  - `event_type` (encoded as dummy variables)\n",
        "  - `severity` (ordinal)\n",
        "  - `season` (derived from date)\n",
        "  - `avg_temp` and `avg_wind_speed` for the event window\n",
        "- **Features from streaming table (`realtime_weather_streaming`):**\n",
        "  - `current_temp_c`, `current_wind_speed`, `current_humidity`\n",
        "  - `current_condition_main` (rain, snow, clear, etc.)\n",
        "- **BigQuery ML model type:** `LOGISTIC_REG` (logistic regression) in BigQuery ML.\n",
        "- **Evaluation:** Use `ML.EVALUATE` to inspect accuracy, precision, recall, and ROC AUC, with a focus on recall\n",
        "  for the high-risk class.\n",
        "- **Usage:** Regularly run `ML.PREDICT` on the latest joined streaming features to generate a table of risk scores\n",
        "  by region and time, which feeds the Looker Studio dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba933eaf",
      "metadata": {
        "id": "ba933eaf"
      },
      "source": [
        "## Section 5 — Dashboard KPIs (Looker Studio)\n",
        "\n",
        "| # | KPI name                               | Why it matters                                                                 | Data source/table                                                          |\n",
        "|---|----------------------------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------|\n",
        "| 1 | % of regions flagged as High-Risk now  | Shows how widespread severe weather risk is at the current moment              | `weather_risk_scores` (latest predictions)                                |\n",
        "| 2 | Avg predicted risk score by hour       | Reveals how risk has changed over the last 24 hours                            | `weather_risk_scores` aggregated by hour                                  |\n",
        "| 3 | Number of severe weather events today  | Indicates how active the weather has been in the operating area                | `historical_weather` filtered to today’s date                             |\n",
        "| 4 | Deliveries delayed during bad weather* | (If delivery data is added) measures real impact of weather on operations      | Join of delivery table with `historical_weather` (optional extension)     |\n",
        "| 5 | Model confidence distribution          | Helps monitor times when the model is uncertain and may need retraining/tuning | `weather_risk_scores` (prediction probabilities)                          |\n",
        "\n",
        "\\*If there is no real delivery data, this KPI can be removed or simulated."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7afcd9f0",
      "metadata": {
        "id": "7afcd9f0"
      },
      "source": [
        "## Challenge — Devil’s Advocate Prompt\n",
        "\n",
        "### 6.1 My Devil’s Advocate Prompt\n",
        "\n",
        "> Play devil’s advocate for my weather-risk monitoring architecture. Identify the single biggest\n",
        "> technical risk or failure point in this design (for example, dependence on the external OpenWeather\n",
        "> API, cost of running a continuous Dataflow job, or maintaining consistent schemas between batch and\n",
        "> streaming tables). Explain why it is risky and propose at least two concrete mitigation strategies\n",
        "> I can implement before and during development.\n",
        "\n",
        "### 6.2 Gemini’s Devil’s Advocate Response (summary)\n",
        "Gemini said the biggest risk in my design is relying too much on the OpenWeather API. If the API stops working, changes its data, or rate-limits me, my whole streaming pipeline could fail. To lower the risk, Gemini suggested adding retry and error-logging so I know when problems happen, and storing backup API results in GCS so data isn’t lost. It also mentioned that Dataflow could cost more if it runs all the time, so I should monitor costs and scale the job only when needed.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}